{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af1cc136-2d1e-470b-a89c-7728d21ef7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4ac6477-bd62-46cb-8933-c5582b7d47e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d265a485-78d0-4f8e-9179-668f8b2f0a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf3483d9-a4ec-4652-8e98-03b699645020",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, BatchNormalization, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8a792cf-2115-4f4a-87ef-84c73766a3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1803f69c-f35d-4015-a551-6ceeb03e9b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\Downloads\\emotion_det\\emotion\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, glob, random\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import torch\n",
    "\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21f2ac58-9bb6-4e5e-90f7-711ce18bfce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be59d5a4-8491-4593-855c-bc192086e4dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54aa708-effc-4669-a15b-57b4aab70689",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09662753-d83c-47c4-8d60-d82befa01a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\python3.11\\Lib\\inspect.py:992: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
      "  if ismodule(module) and hasattr(module, '__file__'):\n",
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_25544\\723198531.py:6: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
      "  from speechbrain.pretrained import SpeakerRecognition\n",
      "2025-06-25 06:23:52.078 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-25 06:23:52.284 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\Dell\\Downloads\\emotion_det\\emotion\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-06-25 06:23:52.286 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-25 06:23:52.288 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-25 06:23:52.289 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-25 06:23:52.289 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-25 06:23:52.291 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-25 06:23:52.292 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-25 06:23:52.294 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-25 06:23:52.295 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-25 06:23:52.297 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-25 06:23:52.298 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-25 06:23:52.300 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import streamlit as st\n",
    "import numpy as np\n",
    "import librosa\n",
    "import joblib\n",
    "from keras.models import load_model\n",
    "import torchaudio\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift\n",
    "\n",
    "int_to_emotion = {\n",
    "    0: 'neutral', 1: 'calm', 2: 'happy', 3: 'sad',\n",
    "    4: 'angry', 5: 'fearful', 6: 'disgust', 7: 'surprised'\n",
    "}\n",
    "\n",
    "\n",
    "# --- Load models and preprocessors ---\n",
    "model_female = load_model(r\"female_models\\female_model.keras\")\n",
    "model_male = load_model(\"male_models\\\\male_model.keras\")\n",
    "scaler_female = joblib.load(\"female_models\\\\female_scaler.pkl\")\n",
    "scaler_male = joblib.load(\"male_models\\\\male_scaler.pkl\")\n",
    "\n",
    "\n",
    "from voice_gender_classifier.model import ECAPA_gender\n",
    "\n",
    "gender_model = ECAPA_gender.from_pretrained(\"JaesungHuh/voice-gender-classifier\")\n",
    "gender_model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gender_model.to(device)\n",
    "\n",
    "def predict_gender(file):\n",
    "    with torch.no_grad():\n",
    "        return gender_model.predict(file, device=device)\n",
    "\n",
    "\n",
    "\n",
    "# --- Your Feature Extraction Function ---\n",
    "def extract_features(audio, sr, n_mfcc=40, n_chroma=12, n_bands=6):\n",
    "    stft = np.abs(librosa.stft(audio))\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc)\n",
    "    mfccs_processed = np.vstack([np.mean(mfccs, axis=1), np.std(mfccs, axis=1), np.median(mfccs, axis=1)])\n",
    "    chroma = librosa.feature.chroma_stft(S=stft, sr=sr, n_chroma=n_chroma)\n",
    "    chroma_processed = np.vstack([np.mean(chroma, axis=1), np.std(chroma, axis=1)])\n",
    "    contrast = librosa.feature.spectral_contrast(S=stft, sr=sr, n_bands=n_bands)\n",
    "    contrast_processed = np.vstack([np.mean(contrast, axis=1), np.std(contrast, axis=1)])\n",
    "    tonnetz = librosa.feature.tonnetz(y=librosa.effects.harmonic(audio), sr=sr)\n",
    "    tonnetz_processed = np.vstack([np.mean(tonnetz, axis=1), np.std(tonnetz, axis=1)])\n",
    "    centroid = librosa.feature.spectral_centroid(y=audio, sr=sr)\n",
    "    bandwidth = librosa.feature.spectral_bandwidth(y=audio, sr=sr)\n",
    "    rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sr)\n",
    "    flatness = librosa.feature.spectral_flatness(y=audio)\n",
    "    spectral_features = np.array([np.mean(centroid), np.std(centroid), np.mean(bandwidth), np.std(bandwidth),\n",
    "                                  np.mean(rolloff), np.std(rolloff), np.mean(flatness), np.std(flatness)])\n",
    "    zero_crossing = librosa.feature.zero_crossing_rate(audio)\n",
    "    rms = librosa.feature.rms(y=audio)\n",
    "    temporal_features = np.array([np.mean(zero_crossing), np.std(zero_crossing), np.mean(rms), np.std(rms)])\n",
    "    mel = librosa.feature.melspectrogram(y=audio, sr=sr)\n",
    "    mel_processed = np.array([np.mean(mel), np.std(mel), np.median(mel), np.max(mel)])\n",
    "    return np.hstack([mfccs_processed.flatten(), chroma_processed.flatten(), contrast_processed.flatten(),\n",
    "                      tonnetz_processed.flatten(), spectral_features, temporal_features, mel_processed])\n",
    "\n",
    "\n",
    "\n",
    "# --- Streamlit UI ---\n",
    "st.title(\"🎙️ Emotion Detection App\")\n",
    "st.markdown(\"Upload a `.wav` file. The system will detect gender and predict the emotion.\")\n",
    "\n",
    "uploaded_file = st.file_uploader(\"Upload your voice (.wav)\", type=[\"wav\"])\n",
    "\n",
    "if uploaded_file is not None:\n",
    "    st.audio(uploaded_file, format='audio/wav')\n",
    "    \n",
    "    # Save to temp file for both torchaudio and librosa use\n",
    "    temp_path = \"temp_audio.wav\"\n",
    "    with open(temp_path, \"wb\") as f:\n",
    "        f.write(uploaded_file.read())\n",
    "    \n",
    "    # Gender prediction\n",
    "    gender = predict_gender(temp_path)\n",
    "    st.write(f\"**Predicted Gender:** `{gender}`\")\n",
    "    \n",
    "    # Feature extraction\n",
    "    \n",
    "    \n",
    "    # Choose model\n",
    "    if gender == 'female':\n",
    "        audio, sr = librosa.load(temp_path, sr=16000)\n",
    "        features = extract_features(audio, sr)\n",
    "        features_scaled = scaler_female.transform([features])\n",
    "        features_input = features_scaled.reshape((1, features_scaled.shape[1], 1))  # CNN shape\n",
    "        probs = model_female.predict(features_input)[0]\n",
    "        predicted_class = int_to_emotion[np.argmax(probs)]\n",
    "    else:\n",
    "        audio, sr = librosa.load(temp_path, sr=16000)\n",
    "        features = extract_features(audio, sr)\n",
    "        features_scaled = scaler_male.transform([features])\n",
    "        features_input = features_scaled.reshape((1, features_scaled.shape[1], 1))  # CNN shape\n",
    "        probs = model_male.predict(features_input)[0]\n",
    "        predicted_class = int_to_emotion[np.argmax(probs)]\n",
    "    # predicted_class = label_encoder.inverse_transform([np.argmax(probs)])[0]\n",
    "    \n",
    "    st.success(f\"**Predicted Emotion:** `{predicted_class}`\")\n",
    "    st.bar_chart(probs)\n",
    "\n",
    "    # Cleanup\n",
    "    os.remove(temp_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329e9cf6-ccc2-440b-925b-872137ce299a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!streamlit run C:\\Users\\Dell\\Downloads\\emotion_det\\emotion\\Lib\\site-packages\\ipykernel_launcher.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037f06a7-4883-4501-affb-8205f69675ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
